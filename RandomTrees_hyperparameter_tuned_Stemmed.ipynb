{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('enron_spam_data.csv')\n",
    "\n",
    "# Sampling the dataset to reduce size\n",
    "df = df.sample(frac=0.1, random_state=42)  # Use 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regexClean(message_content):\n",
    "    message_content = str(message_content) \n",
    "    message_content = re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\\\./]+', ' ', message_content) \n",
    "    message_content = re.sub(r'[A-Za-z0-9._\\\\-]+@[A-Za-z0-9-]*\\\\.[a-z]{2,3}', '', message_content)\n",
    "    message_content = re.sub(r'<[^<]+?>', '', message_content) \n",
    "    message_content = message_content.replace('\\n', ' ')\n",
    "    message_content = message_content.lower()\n",
    "    return message_content\n",
    "\n",
    "df['Cleaned_Message'] = df['Message'].apply(regexClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def punct_removal(message_content):\n",
    "    message_content = str(message_content)\n",
    "    message_content = re.sub(r'\\d+', ' ', message_content)\n",
    "    message_content = message_content.replace(r'[^a-zA-Z]', '') \n",
    "    message_content = message_content.translate(str.maketrans('', '', punctuation))\n",
    "    return message_content\n",
    "\n",
    "df_with_punct_numb_removed = df.copy()\n",
    "df_with_punct_numb_removed['Cleaned_Message'] = df['Cleaned_Message'].apply(punct_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def stem_stopword_rem(message_content):\n",
    "    message_content = str(message_content)\n",
    "    stop_words = stopwords.words('english')\n",
    "    words = [eachword for eachword in message_content.split() if eachword not in stop_words]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stemmed = [stemmer.stem(eachword) for eachword in words]\n",
    "    return ' '.join(stemmed)\n",
    "\n",
    "df_stem_stopword_removed = df_with_punct_numb_removed.copy()\n",
    "df_stem_stopword_removed['Cleaned_Message'] = df_with_punct_numb_removed['Cleaned_Message'].apply(stem_stopword_rem)\n",
    "display(df_stem_stopword_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "X = vectorizer.fit_transform(df_stem_stopword_removed['Cleaned_Message']).toarray()\n",
    "\n",
    "# Convert the vectorized data to a DataFrame for better visualization\n",
    "vectorized_df = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "print(vectorized_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "y = df['Spam/Ham'] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for Grid Search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth': [4, 5, 6, 7, 8],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the best model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(y_test, y_pred, class_labels=None, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = [0, 1]\n",
    "    plt.xticks(tick_marks, ['Ham', 'Spam'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['Ham', 'Spam'])\n",
    "    \n",
    "    thresh = conf_matrix.max() / 2.\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_confusion_matrix(y_test, y_pred, class_labels=None, title='Random Trees on stemmed data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob, pos_label='spam')\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:0.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "indices = sorted(range(len(importances)), key=lambda i: importances[i])[-10:]  # Top 10 features\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(range(len(indices)), [importances[i] for i in indices], align='center')\n",
    "plt.yticks(range(len(indices)), [vectorizer.get_feature_names_out()[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
